import os
import glob
import json
import re
import time
import asyncio
import httpx
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from supabase import create_client, Client

# Load environment variables
load_dotenv(override=True)

# Configuration
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
EMBEDDING_MODEL = "text-embedding-3-large"
PATTERN_DIR = os.path.join(os.path.dirname(__file__), "g9_rag") # Default to current dir/g9_rag if not found
if not os.path.exists(PATTERN_DIR):
    PATTERN_DIR = os.path.dirname(__file__) # Fallback to current dir

# Initialize Supabase
if not SUPABASE_URL or not SUPABASE_KEY:
    # Try manual parsing if load_dotenv failed
    try:
        with open(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'), 'r') as f:
            for line in f:
                if '=' in line:
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value
        SUPABASE_URL = os.getenv("SUPABASE_URL")
        SUPABASE_KEY = os.getenv("SUPABASE_KEY")
        OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    except Exception as e:
        print(f"‚ö†Ô∏è Manual .env load failed: {e}")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("Missing Supabase credentials")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Initialize HTTP Client for OpenRouter
headers = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "https://github.com/gate9/g9-rag",
    "X-Title": "G9 Pattern Engine",
    "Content-Type": "application/json"
}

# Hardcoded Title Map (User Provided)
TITLE_MAP = {
    "P-001": "Í∏àÎ¶¨ Í∏âÎì± (Interest Rate Spike)",
    "P-002": "Ïú†Í∞Ä Í∏âÎì± (Oil Price Spike)",
    "P-003": "Îã¨Îü¨ Í∞ïÏÑ∏ (Dollar Super-Strength)",
    "P-004": "ÏóîÏ†Ä Ïã¨Ìôî (JPY Structural Weakening)",
    "P-005": "Ïù∏ÌîåÎ†àÏù¥ÏÖò Í∏âÎì± (Inflation Spike)",
    "P-006": "Í≤ΩÍ∏∞ Ïπ®Ï≤¥ ÏãúÍ∑∏ÎÑê (Recession Signal)",
    "P-007": "Í∏∞Ïà†Ï£º Î≤ÑÎ∏î Î∂ïÍ¥¥ (Tech Bubble Burst)",
    "P-008": "ÏßÄÏ†ïÌïôÏ†Å Ï†ÑÏüÅ ÏúÑÍ∏∞ (Geopolitical War)",
    "P-009": "Ïã†Ïö© Í≤ΩÏÉâ Î∞è Ïú†ÎèôÏÑ± ÏúÑÍ∏∞ (Credit Crunch)",
    "P-010": "Ï§ëÍµ≠ Í≤ΩÍ∏∞ ÎëîÌôî Î∞è Î∂ÄÏñëÏ±Ö (China Slowdown/Stimulus)",
    "P-011": "Ïú†ÎèôÏÑ± ÏÇ¨Ïù¥ÌÅ¥ Í∏âÎ≥Ä (Liquidity Shock)",
    "P-012": "Í≥†Ïö© ÏãúÏû• Ï∂©Í≤© (Labor Market Shock)",
    "P-013": "Í∏àÏúµ ÏãúÏä§ÌÖú Ïä§Ìä∏Î†àÏä§ (Financial Stress)",
    "P-014": "Í≥µÍ∏âÎßù Î∞è ÏõêÏûêÏû¨ ÎåÄÎûÄ (Supply Chain Crisis)",
    "P-015": "ÏÑ†Í±∞ Î∞è Ï†ïÏπòÏ†Å Î∂àÌôïÏã§ÏÑ± (Political Uncertainty)",
    "P-016": "ÏÜåÎπÑ ÏúÑÏ∂ï Î∞è ÏÜåÎß§ ÌåêÎß§ ÏáºÌÅ¨ (Consumption Shock)",
    "P-017": "Íµ≠Ï±Ñ Í∏àÎ¶¨ Ïó≠Ï†Ñ Î∞è Ï†ïÏÉÅÌôî (Yield Curve Inversion)",
    "P-018": "Ï∫êÎ¶¨ Ìä∏Î†àÏù¥Îìú Ï≤≠ÏÇ∞ (Carry Trade Unwind)",
    "P-019": "Ïã†Ìù•Íµ≠ ÏûêÍ∏à Ïú†Ï∂ú (EM Capital Flight)",
    "P-020": "Ï§ëÏïôÏùÄÌñâ Ï†ïÏ±Ö Ï†ÑÌôò (Central Bank Pivot)",
    "P-021": "ÏñëÏ†ÅÏôÑÌôî/Í∏¥Ï∂ï ÏÇ¨Ïù¥ÌÅ¥ (QE/QT Cycle)",
    "P-022": "Îã¨Îü¨ Ïú†ÎèôÏÑ± Í≤ΩÏÉâ (USD Funding Stress)",
    "P-023": "ÏãúÏû• Í≥µÌè¨ Î∞è Ìà¨Îß§ (VIX/Capitulation)",
    "P-024": "Î¶¨Ïä§ÌÅ¨ Ïò®/Ïò§ÌîÑ Ï†ÑÌôò (Risk On/Off)",
    "P-025": "ÎØ∏Ï§ë Î¨¥Ïó≠ Ï†ÑÏüÅ (US-China Trade War)",
    "P-026": "Ï§ëÎèô ÏßÄÏ†ïÌïô Î¶¨Ïä§ÌÅ¨ (Middle East Risk)",
    "P-027": "Ïú†ÎüΩ ÏóêÎÑàÏßÄ ÏïàÎ≥¥ ÏúÑÍ∏∞ (EU Energy Crisis)",
    "P-028": "ÎåÄÎßå Ìï¥Ìòë Î¶¨Ïä§ÌÅ¨ (Taiwan Flashpoint)",
    "P-029": "Í∏ÄÎ°úÎ≤å Ï†úÏû¨ Î∞è ÏàòÏ∂ú ÌÜµÏ†ú (Global Sanctions)",
    "P-030": "Ïã†Ìù•Íµ≠ Î∂ÄÏ±Ñ ÏúÑÍ∏∞ (Sovereign Debt Crisis)",
    "P-031": "Î∞òÎèÑÏ≤¥ ÏÇ¨Ïù¥ÌÅ¥ (Semiconductor Cycle)",
    "P-032": "Ï†ÑÍ∏∞Ï∞®(EV) Í≥ºÏó¥‚ÜîÏπ®Ï≤¥ ÏÇ¨Ïù¥ÌÅ¥ (EV Boom-Bust Cycle)",
    "P-033": "Î∞îÏù¥Ïò§¬∑Ï†úÏïΩ ÏûÑÏÉÅ / FDA ÏÇ¨Ïù¥ÌÅ¥ (Bio/Pharma Cycle)",
    "P-034": "Í∏àÏúµÏ£º Ïä§Ìä∏Î†àÏä§(ÏùÄÌñâ¬∑Î≥¥Ìóò) Ìå®ÌÑ¥ (Financial Sector Stress)",
    "P-035": "ÎπÖÌÖåÌÅ¨ Í∑úÏ†ú/Î∞òÎèÖÏ†ê Ìå®ÌÑ¥ (Tech Regulation)",
    "P-036": "Í±¥ÏÑ§¬∑Ïù∏ÌîÑÎùº ÏÇ¨Ïù¥ÌÅ¥ (Construction & Infrastructure)",
    "P-037": "Ìï≠Í≥µ¬∑Í¥ÄÍ¥ë Î¶¨Î∞îÏö¥Îìú (Tourism Rebound)",
    "P-038": "Ïª§Î®∏ÎîîÌã∞ ÏäàÌçºÏÇ¨Ïù¥ÌÅ¥ (Commodity Supercycle)",
    "P-039": "ÏÜåÎπÑÏû¨ ÍµêÏ≤¥ Ï£ºÍ∏∞ (Replacement Cycle)",
    "P-040": "Î∞©ÏÇ∞¬∑Íµ≠Î∞© ÏóÖÏÇ¨Ïù¥ÌÅ¥ (Defense Upswing)",
    "P-041": "Í∏ÄÎ°úÎ≤å ÏûêÍ∏à ÏÑπÌÑ∞ Î°úÌÖåÏù¥ÏÖò (Global Sector Rotation)",
    "P-042": "ÌôòÏú® Î∂ïÍ¥¥ / ÌÜµÌôîÏúÑÍ∏∞ (Currency Crisis)",
    "P-043": "ÏïàÏ†ÑÏûêÏÇ∞ Ïè†Î¶º (Flight to Safety)",
    "P-044": "ÏõêÏûêÏû¨ ÌÜµÌôî ÏÇ¨Ïù¥ÌÅ¥ (Commodity Currency Cycle)",
    "P-045": "Í≤ΩÏ†úÏßÄÌëú ÏÑúÌîÑÎùºÏù¥Ï¶à/ÏáºÌÅ¨ (Macro Surprise)",
    "P-046": "Ïû•Îã®Í∏∞ Í∏àÎ¶¨ Ïä§ÌîÑÎ†àÎìú Ìä∏Î†àÏù¥Îìú (Yield Curve Trade)",
    "P-047": "AI ÏÑúÎ∏åÏóÖÏ¢Ö Í≥ºÏó¥ (AI Mini-Bubble)",
    "P-048": "Meme Stock / Í∞úÏù∏Ìà¨ÏûêÏûê Í¥ëÌíç (Retail Mania)",
    "P-049": "ÏãúÏû• Ìà¨Îß§ Î∞îÎã• Ìå®ÌÑ¥ (Market Capitulation)",
    "P-050": "Î∏îÎûôÏä§ÏôÑ / ÎØ∏ÏßÄÏùò Í≥µÌè¨ (Black Swan Event)" 
}

def parse_markdown(file_path: str) -> Tuple[Dict[str, Any], List[str]]:
    """Parses markdown and returns data dict and list of corrections made."""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    filename = os.path.basename(file_path)
    pattern_id = filename.replace('.md', '')
    corrections = []

    # 1. Category
    category_match = re.search(r'(?:Category|Ïπ¥ÌÖåÍ≥†Î¶¨Î™Ö|Category\*\*)\s*[:]\s*(.+)', content, re.IGNORECASE)
    category = "Uncategorized"
    if category_match:
        raw_cat = category_match.group(1).strip()
        category = re.sub(r'[\*_]', '', raw_cat).strip()
    else:
        corrections.append("Category missing -> Set to 'Uncategorized'")

    # 2. Title (FORCE OVERRIDE from MAP)
    if pattern_id in TITLE_MAP:
        title = TITLE_MAP[pattern_id]
        # Check if parsed title matches (just for logging)
        # ... (skipping check to save time, map is authority)
    else:
        # Fallback to parsing if not in map (should not happen for P-001 to P-050)
        title = ""
        # Strategy A: **Ìå®ÌÑ¥Î™Ö:** or Ìå®ÌÑ¥Î™Ö:
        title_match_bold = re.search(r'(?:\*\*|)?(?:Ìå®ÌÑ¥Î™Ö|Ìå®ÌÑ¥Ï†úÎ™©)(?:\*\*|)?\s*[:]\s*(.+)', content)
        if title_match_bold:
            title = title_match_bold.group(1).strip()
        
        # Strategy B: # **P-XXX ‚Äî Title** or P-XXX ‚Äî Title
        if not title:
            title_match_header = re.search(r'(?:#\s*)?(?:\*\*)?P-\d+\s*[‚Äî‚Äì-]\s*(.+?)(?:\*\*)?(?:\n|$)', content)
            if title_match_header:
                title = title_match_header.group(1).strip()
                
        # Strategy C: JSON block "name": "..."
        if not title:
            json_name_match = re.search(r'"name"\s*:\s*"(.+?)"', content)
            if json_name_match:
                title = json_name_match.group(1).strip()

        if not title:
            title = pattern_id
            corrections.append(f"Title missing -> Set to '{pattern_id}'")

    # 3. Core
    core = ""
    core_match_sec8 = re.search(r'(?:Embedding Core|ÌïµÏã¨ Î¨∏Ïû•).+?\n+(?:> )?(?:\*\*)?["‚Äú](.+?)["‚Äù](?:\*\*)?', content, re.DOTALL | re.IGNORECASE)
    if not core_match_sec8:
         core_match_sec8 = re.search(r'(?:Embedding Core|ÌïµÏã¨ Î¨∏Ïû•).+?\n+(?:> )?(.+)', content, re.DOTALL | re.IGNORECASE)
    
    if not core_match_sec8:
        core_match_def = re.search(r'(?:Core Definition|Î≥∏Ïßà Ï†ïÏùò).+?\n+(?:> )?(?:\*\*)?["‚Äú](.+?)["‚Äù](?:\*\*)?', content, re.DOTALL | re.IGNORECASE)
        if core_match_def:
            core_match_sec8 = core_match_def

    if core_match_sec8:
        raw_core = core_match_sec8.group(1).replace('\n', ' ').strip()
        core = re.sub(r'[\*_]', '', raw_core).strip()
    else:
        json_core_match = re.search(r'"core_logic"\s*:\s*"(.+?)"', content)
        if json_core_match:
             core = json_core_match.group(1).strip()
    
    if not core:
        corrections.append("Core missing -> Set to empty string")

    # 4. Triggers
    triggers = []
    trigger_section = re.search(r'(?:Trigger Sentences|Ìå®ÌÑ¥ Ìä∏Î¶¨Í±∞).+?(?:##|\Z)', content, re.DOTALL | re.IGNORECASE)
    if trigger_section:
        trigger_text = trigger_section.group(0)
        items = re.findall(r'(?:-|‚Ä¢)\s*["‚Äú](.+?)["‚Äù]', trigger_text)
        if not items:
             items = re.findall(r'(?:-|‚Ä¢)\s*(.+)', trigger_text)
        triggers = [re.sub(r'[\*_]', '', item).strip() for item in items if item.strip()]
    
    if not triggers:
        corrections.append("Triggers missing -> Set to empty list")

    # 5. SQL Rules
    sql_rules = ""
    sql_section = re.search(r'(?:SQL Rules|SQL Í≤ÄÏÉâ Í∑úÏπô).+?(?:##|\Z)', content, re.DOTALL | re.IGNORECASE)
    if sql_section:
        sql_match = re.search(r'```(?:sql)?\s*(.+?)\s*```', sql_section.group(0), re.DOTALL)
        if sql_match:
            sql_rules = sql_match.group(1).strip()
        else:
            raw_sql_match = re.search(r'(SELECT.+?;)', sql_section.group(0), re.DOTALL | re.IGNORECASE)
            if raw_sql_match:
                sql_rules = raw_sql_match.group(1).strip()
    
    if not sql_rules:
        json_sql_match = re.search(r'"sql_rule"\s*:\s*"(.+?)"', content)
        if json_sql_match:
            sql_rules = json_sql_match.group(1).strip()
    
    if not sql_rules:
        corrections.append("SQL Rules missing -> Set to empty string")

    return {
        "pattern_id": pattern_id,
        "category": category,
        "title": title,
        "core": core,
        "triggers": triggers,
        "sql_rules": sql_rules,
        "full_text": content
    }, corrections

async def get_embedding(text: str, retries=3) -> List[float]:
    """Generates embedding using OpenRouter."""
    for attempt in range(retries):
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OPENROUTER_BASE_URL}/embeddings",
                    headers=headers,
                    json={
                        "model": EMBEDDING_MODEL,
                        "input": text
                    },
                    timeout=30.0
                )
                response.raise_for_status()
                data = response.json()
                return data['data'][0]['embedding']
        except Exception as e:
            if attempt == retries - 1:
                print(f"‚ùå Embedding failed: {e}")
                raise
            time.sleep(2 ** attempt)
    return []

async def process_file(file_path: str):
    data, corrections = parse_markdown(file_path)
    
    # Construct Embedding Input: Full Text + Core + Triggers
    embed_input = f"{data['full_text']}\n\nCore: {data['core']}\n\nTriggers: {', '.join(data['triggers'])}"
    
    try:
        embedding = await get_embedding(embed_input)
        
        # Upsert to Supabase
        record = {
            "pattern_id": data['pattern_id'],
            "category": data['category'],
            "title": data['title'],
            "core": data['core'],
            "triggers": data['triggers'],
            "sql_rules": data['sql_rules'],
            "full_text": data['full_text'],
            "embedding": embedding
        }
        
        supabase.table("macro_patterns").upsert(record).execute()
        
        status = "‚úÖ Synced"
        if corrections:
            status = "‚ö†Ô∏è  Fixed"
        
        print(f"{data['pattern_id']:<10} {status:<10} {len(embedding):<6} {data['title'][:30]:<30}")
        if corrections:
            for c in corrections:
                print(f"   ‚Ü≥ {c}")

    except Exception as e:
        print(f"‚ùå {data['pattern_id']} Failed: {e}")

async def main():
    print("üöÄ Starting Pattern Repair Pipeline (with Hardcoded Titles)...")
    print(f"üìÇ Pattern Directory: {PATTERN_DIR}")
    
    files = sorted(glob.glob(os.path.join(PATTERN_DIR, "P-*.md")))
    print(f"üìÇ Found {len(files)} files")
    
    print("-" * 80)
    print(f"{'ID':<10} {'Status':<10} {'Dim':<6} {'Title':<30}")
    print("-" * 80)

    for file_path in files:
        await process_file(file_path)
        
    print("-" * 80)
    print("‚ú® Pattern Repair Complete.")

if __name__ == "__main__":
    asyncio.run(main())
