import os
import asyncio
import json
import httpx
from typing import List, Dict, Any
from dotenv import load_dotenv
from supabase import create_client, Client

# === CONFIG ===
load_dotenv(override=True)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
MODEL_NAME = "openai/gpt-4o"

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("Missing Supabase credentials")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Initialize HTTP Client for OpenRouter
headers = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "https://github.com/gate9/g9-rag",
    "X-Title": "G9 Ticker Mapper",
    "Content-Type": "application/json"
}

BATCH_SIZE = 1000
CONFIDENCE_THRESHOLD_UPDATE = 0.85

async def extract_tickers_batch(news_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Extracts tickers for a batch of news items using LLM.
    """
    prompt_items = []
    for item in news_items:
        # Robust text extraction
        raw_text = item.get('raw_text') or item.get('summary') or ""
        if not isinstance(raw_text, str): raw_text = ""
        
        text_content = f"Title: {item.get('title', '')}\nKeywords: {item.get('keywords', '')}\nText: {raw_text[:300]}"
        prompt_items.append({"id": item['id'], "content": text_content})

    prompt_text = json.dumps(prompt_items, indent=2)
    
    system_prompt = """
    ë„ˆì˜ ì—­í• ì€ "í‹°ì»¤ ë³µì› ì—”ì§„(Ticker Restoration Engine)"ì´ë‹¤.
    global_news_all í…Œì´ë¸” ë‚´ ticker í•„ë“œê°€ NULLì¸ ë‰´ìŠ¤ë“¤ì„ ìë™ ë¶„ì„í•˜ê³ 
    ì œëª©(title), summary, keywords, raw_text ì¤‘ ê¸°ì—…ëª…/ì§€ëª…/ê¸°ê´€ëª…ì„ íƒì§€í•˜ì—¬
    í•´ë‹¹ ê¸°ì—…ì˜ ì‹¤ì œ ì£¼ì‹ í‹°ì»¤ë¥¼ ì¶”ë¡ í•œ ë’¤ ë‹¤ì‹œ DBì— ë§¤í•‘í•´ì•¼ í•œë‹¤.

    ì¤‘ìš”í•œ ì›ì¹™
    1) tickerê°€ NULLì´ë©´ ë°˜ë“œì‹œ candidate_ticker 1~3ê°œë¥¼ ì œì•ˆí•œë‹¤.
    2) í™•ì •ì´ ì–´ë ¤ìš¸ ê²½ìš° "unknown"ì´ ì•„ë‹ˆë¼ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ confidence ì ìˆ˜ì™€ í•¨ê»˜ ì œì‹œí•œë‹¤.
    3) ë§¤ìš° í™•ì‹¤í•  ê²½ìš° confidence ê¸°ì¤€ì´ 0.85 ì´ìƒì´ë©´ 1ê°œ tickerë¡œ ë‹¨ì¼ í™•ì •í•œë‹¤.
    4) ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì— ëª…ì‹œë˜ì§€ ì•Šì€ ê¸°ì—…ëª…ì€ ì–µì§€ë¡œ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.
    5) êµ­ê°€/ì‹œì¥/ì§€ìˆ˜ì™€ í•¨ê»˜ ë“±ì¥í•˜ëŠ”ì§€ â†’ ë§¤ì¹­ ì •í™•ë„ ìƒìŠ¹
       ì˜ˆ) "ì‚¼ì„±" + "KOSPI" â†’ 005930
           "NVIDIA" + "NASDAQ" â†’ NVDA
           "Sony" + "Tokyo" â†’ 6758.T
    6) ì •ë¶€Â·ë‹¨ì²´Â·ê¸°ê´€ì€ ê¸°ì—…ì´ ì•„ë‹ˆë¯€ë¡œ ë¬´ì¡°ê±´ ì œì™¸
       (UN, NATO, WHO, ì •ë¶€ë¶€ì²˜ ë“±)

    DB ì—…ë°ì´íŠ¸ ëª©ì  JSON OUTPUT FORMAT
    Return a JSON object with a key "results" containing a list of objects:
    {
      "id": "<uuid>",                       // ë‰´ìŠ¤ row id
      "title": "<headline>",                // ì°¸ê³ í•œ ì œëª©
      "company": "<Detected Company Name>", // ê¸°ì—…ëª…
      "candidate_tickers": ["AAPL","GOOG"], // í›„ë³´ ë¦¬ìŠ¤íŠ¸
      "confidence": 0.79,                   // 0~1.0 ì ìˆ˜
      "final": "AAPL",                      // ìë™ í™•ì •(ì—†ìœ¼ë©´ null)
      "reasoning": "ë³¸ë¬¸ì—ì„œ Apple ì œí’ˆ ì–¸ê¸‰ ë° NASDAQ ë¬¸ë§¥"
    }
    """

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Analyze these news items:\n{prompt_text}"}
        ],
        "response_format": {"type": "json_object"}
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{OPENROUTER_BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                timeout=120.0
            )
            response.raise_for_status()
            result = response.json()
            content = result['choices'][0]['message']['content']
            parsed = json.loads(content)
            return parsed.get('results', [])
    except Exception as e:
        print(f"âŒ LLM Batch Error: {e}")
        return []

async def process_batch():
    total_processed = 0
    total_mapped = 0
    new_tickers = set()
    low_confidence_companies = []

    while True:
        print(f"\nğŸ”„ Fetching batch of {BATCH_SIZE} NULL tickers...")
        
        # Fetch NULL tickers that haven't been checked yet
        response = supabase.table("global_news_all").select("*").is_("ticker", "null").eq("ai_ticker_checked", False).limit(BATCH_SIZE).execute()
        
        if not response.data:
             response = supabase.table("global_news_all").select("*").eq("ticker", "").eq("ai_ticker_checked", False).limit(BATCH_SIZE).execute()
        
        news_items = response.data
        if not news_items:
            print("âœ¨ No more unchecked NULL tickers found. Process Complete.")
            break
            
        print(f"   Processing {len(news_items)} items...")
        
        chunk_size = 50
        results = []
        
        for i in range(0, len(news_items), chunk_size):
            chunk = news_items[i:i+chunk_size]
            print(f"   - Analyzing chunk {i}-{i+len(chunk)}...")
            chunk_results = await extract_tickers_batch(chunk)
            results.extend(chunk_results)
            
        updates_count = 0
        
        # Mark all processed items as checked
        processed_ids = [item['id'] for item in news_items]
        if processed_ids:
            try:
                # Update ai_ticker_checked = True for all processed IDs
                # Supabase-py .in_() filter
                supabase.table("global_news_all").update({"ai_ticker_checked": True}).in_("id", processed_ids).execute()
            except Exception as e:
                print(f"   âš ï¸ Failed to mark items as checked: {e}")

        for res in results:
            news_id = res['id']
            # ... rest of processing ...
        
        for res in results:
            news_id = res['id']
            # title = res.get('title')
            candidates = res.get('candidate_tickers', [])
            conf = res.get('confidence', 0.0)
            final_ticker = res.get('final')
            reasoning = res.get('reasoning')
            
            # Save to ticker_ai_labels
            # We need to handle the unique constraint.
            # If final_ticker is present, use it as 'ticker' for the unique key?
            # Or should we store one row per candidate?
            # The user said "candidate_tickers 1~3ê°œë¥¼ ì œì•ˆí•œë‹¤".
            # And "DB ì €ì¥ ê·œì¹™" -> "ticker_ai_labelsì— ë¨¼ì € ì ì¬".
            # I'll store the primary candidate or final ticker as 'ticker' and the full list in 'candidate_tickers'.
            
            primary_ticker = final_ticker if final_ticker else (candidates[0] if candidates else None)
            company_name = res.get('company') or (candidates[0] if candidates else "Unknown")
            
            if company_name and company_name != "Unknown":
                 try:
                    supabase.table("ticker_ai_labels").upsert({
                        "company": company_name,
                        "ticker": primary_ticker,
                        "candidate_tickers": candidates,
                        "reasoning": reasoning,
                        "confidence": conf
                    }, on_conflict="company,ticker").execute()
                 except Exception as e:
                    pass

            # Update global_news_all
            if conf >= CONFIDENCE_THRESHOLD_UPDATE and final_ticker:
                try:
                    supabase.table("global_news_all").update({"ticker": final_ticker}).eq("id", news_id).execute()
                    updates_count += 1
                    total_mapped += 1
                    new_tickers.add(final_ticker)
                except Exception as e:
                    print(f"   âš ï¸ Update failed for {news_id}: {e}")
            
            if conf < CONFIDENCE_THRESHOLD_UPDATE:
                low_confidence_companies.append((str(candidates), conf))

        total_processed += len(news_items)
        print(f"   âœ… Batch Complete. Updated {updates_count} rows.")

    print("\n" + "="*40)
    print("ğŸ“Š Ticker Regeneration Report")
    print("="*40)
    print(f"Total Analyzed: {total_processed}")
    print(f"Successfully Mapped: {total_mapped}")
    print(f"Success Rate: {(total_mapped/total_processed*100) if total_processed else 0:.1f}%")
    
    print("\nğŸ†• New Tickers Found (Top 20):")
    for t in list(new_tickers)[:20]:
        print(f"- {t}")

if __name__ == "__main__":
    asyncio.run(process_batch())
